{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_pytorch_computer_vision_exercises.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e59f392ab0a4456aab9ad16272f04010": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_877ee0af6b744f979394fc6ee0a19ab2",
              "IPY_MODEL_6485855357c24b50a603e5ffa00b441c",
              "IPY_MODEL_d4d3b5da139f4f56bf0dd5d0f3c7d5ab"
            ],
            "layout": "IPY_MODEL_a889b8e790e24c9baba4a5e379ac9e63"
          }
        },
        "877ee0af6b744f979394fc6ee0a19ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dff622a6496248c6bd19831da7bd3d56",
            "placeholder": "​",
            "style": "IPY_MODEL_8151abf488cd42bc8c5a0f9351c5e659",
            "value": "100%"
          }
        },
        "6485855357c24b50a603e5ffa00b441c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a2901ddea88411a9c8ca93be0b658ef",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d2d2016cd721480b89183ab1ad7ed3a6",
            "value": 5
          }
        },
        "d4d3b5da139f4f56bf0dd5d0f3c7d5ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d002d7967284ddc99ab266787d3fc4b",
            "placeholder": "​",
            "style": "IPY_MODEL_c20ada00198748ecad0c5229419d6916",
            "value": " 5/5 [04:11&lt;00:00, 50.94s/it]"
          }
        },
        "a889b8e790e24c9baba4a5e379ac9e63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dff622a6496248c6bd19831da7bd3d56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8151abf488cd42bc8c5a0f9351c5e659": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a2901ddea88411a9c8ca93be0b658ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2d2016cd721480b89183ab1ad7ed3a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d002d7967284ddc99ab266787d3fc4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c20ada00198748ecad0c5229419d6916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zoezyh/Pytorch-Practice/blob/main/exercises/03_pytorch_computer_vision_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 03. PyTorch Computer Vision Exercises\n",
        "\n",
        "The following is a collection of exercises based on computer vision fundamentals in PyTorch.\n",
        "\n",
        "They're a bunch of fun.\n",
        "\n",
        "You're going to get to write plenty of code!\n",
        "\n",
        "## Resources\n",
        "\n",
        "1. These exercises are based on [notebook 03 of the Learn PyTorch for Deep Learning course](https://www.learnpytorch.io/03_pytorch_computer_vision/).\n",
        "2. See a live [walkthrough of the solutions (errors and all) on YouTube](https://youtu.be/_PibmqpEyhA).\n",
        "  * **Note:** Going through these exercises took me just over 3 hours of solid coding, so you should expect around the same.\n",
        "3. See [other solutions on the course GitHub](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/solutions)."
      ],
      "metadata": {
        "id": "Vex99np2wFVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaeYzOTLwWh2",
        "outputId": "6e25f32e-0685-4293-ff33-bdfaad347d11"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import torch\n",
        "import torch\n",
        "\n",
        "# Exercises require PyTorch > 1.10.0\n",
        "print(torch.__version__)\n",
        "\n",
        "# TODO: Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device_cpu = \"cpu\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNwZLMbCzJLk",
        "outputId": "9464ea6e-327f-4f43-e240-ca201bb4ca9f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. What are 3 areas in industry where computer vision is currently being used?"
      ],
      "metadata": {
        "id": "FSFX7tc1w-en"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Camera and photo apps use computer vision to enhance and sort images.\n",
        "\n",
        "Modern cars use computer vision to avoid other cars and stay within lane lines.\n",
        "\n",
        "Manufacturers use computer vision to identify defects in various products."
      ],
      "metadata": {
        "id": "VyWRkvWGbCXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Search \"what is overfitting in machine learning\" and write down a sentence about what you find."
      ],
      "metadata": {
        "id": "oBK-WI6YxDYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting means creating a model that matches (memorizes) the training set so closely that the model fails to make correct predictions on new data."
      ],
      "metadata": {
        "id": "d1rxD6GObCqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Search \"ways to prevent overfitting in machine learning\", write down 3 of the things you find and a sentence about each.\n",
        "> **Note:** there are lots of these, so don't worry too much about all of them, just pick 3 and start with those."
      ],
      "metadata": {
        "id": "XeYFEqw8xK26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Train with more data.\n",
        "\n",
        "2.Feature selection.\n",
        "\n",
        "3.Regularization."
      ],
      "metadata": {
        "id": "ocvOdWKcbEKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Spend 20-minutes reading and clicking through the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/).\n",
        "\n",
        "* Upload your own example image using the \"upload\" button on the website and see what happens in each layer of a CNN as your image passes through it."
      ],
      "metadata": {
        "id": "DKdEEFEqxM-8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TqZaJIRMbFtS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Load the [`torchvision.datasets.MNIST()`](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) train and test datasets."
      ],
      "metadata": {
        "id": "lvf-3pODxXYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import torch\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Import torchvision\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Check versions\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)\n",
        "\n",
        "# Setup training data\n",
        "train_data = datasets.MNIST(\n",
        "  root=\"data\",\n",
        "  train=True,\n",
        "  download=True,\n",
        "  transform=torchvision.transforms.ToTensor(),\n",
        "  target_transform=None\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "  root=\"data\",\n",
        "  train=False,\n",
        "  download=True,\n",
        "  transform=ToTensor(),\n",
        "  target_transform=None\n",
        ")\n",
        "\n",
        "class_names = train_data.classes"
      ],
      "metadata": {
        "id": "SHjeuN81bHza",
        "outputId": "d0ea3303-3741-4b7b-ec6b-1ddaf99138f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu124\n",
            "0.20.1+cu124\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 54.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.77MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.32MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Visualize at least 5 different samples of the MNIST training dataset."
      ],
      "metadata": {
        "id": "qxZW-uAbxe_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "train_samples = []\n",
        "train_labels = []\n",
        "for sample, label in random.sample(list(train_data), k=9):\n",
        "  train_samples.append(sample)\n",
        "  train_labels.append(label)\n",
        "\n",
        "# View the first sample shape\n",
        "train_samples[0].shape\n",
        "\n",
        "plt.figure(figsize=(9,9))\n",
        "nrows = 3\n",
        "ncols = 3\n",
        "for i, sample in enumerate(train_samples):\n",
        "  # Create subplot\n",
        "  plt.subplot(nrows, ncols, i+1)\n",
        "\n",
        "  # Plot the target image\n",
        "  plt.imshow(sample.squeeze(),cmap=\"gray\")\n",
        "  plt.axis(False)"
      ],
      "metadata": {
        "id": "QVFsYi1PbItE",
        "outputId": "30adffcc-f270-43ac-eb97-5494de32a496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 900x900 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Turn the MNIST train and test datasets into dataloaders using `torch.utils.data.DataLoader`, set the `batch_size=32`."
      ],
      "metadata": {
        "id": "JAPDzW0wxhi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Setup the batch size hyperparameter\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Turn datasets into iterables(batches)\n",
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "               batch_size=BATCH_SIZE,\n",
        "               shuffle=False)\n",
        "\n",
        "train_dataloader, test_dataloader\n",
        "\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "ALA6MPcFbJXQ",
        "outputId": "40518964-c6f5-4180-bad9-8c4fb7c1d774",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7cebfcd19510>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7cebfcbabb50>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Recreate `model_2` used in notebook 03 (the same model from the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/), also known as TinyVGG) capable of fitting on the MNIST dataset."
      ],
      "metadata": {
        "id": "bCCVfXk5xjYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TinyVGG\n",
        "class TinyVGG(nn.Module):\n",
        "  def __init__(self, input_shape:int, hidden_units:int, output_shape:int):\n",
        "    super().__init__()\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "      # Create a conv layer\n",
        "      nn.Conv2d(in_channels=input_shape,\n",
        "           out_channels=hidden_units,\n",
        "           kernel_size=3,\n",
        "           stride=1,\n",
        "           padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=hidden_units,\n",
        "           out_channels=hidden_units,\n",
        "           kernel_size=3,\n",
        "           stride=1,\n",
        "           padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=hidden_units,\n",
        "           out_channels=hidden_units,\n",
        "           kernel_size=3,\n",
        "           stride=1,\n",
        "           padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=hidden_units,\n",
        "           out_channels=hidden_units,\n",
        "           kernel_size=3,\n",
        "           stride=1,\n",
        "           padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=hidden_units*7*7,# Trick!\n",
        "             out_features=output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv_block_1(x)\n",
        "    # print(f\"Output shape of conv_block_1:{x.shape}\")\n",
        "    x = self.conv_block_2(x)\n",
        "    # print(f\"Output shape of conv_block_2:{x.shape}\")\n",
        "    x = self.classifier(x)\n",
        "    # print(f\"Output shape of classifier:{x.shape}\")\n",
        "    return(x)"
      ],
      "metadata": {
        "id": "5IKNF22XbKYS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Train the model you built in exercise 8. for 5 epochs on CPU and GPU and see how long it takes on each."
      ],
      "metadata": {
        "id": "sf_3zUr7xlhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model_cpu = TinyVGG(input_shape=1,\n",
        "         hidden_units=10,\n",
        "         output_shape=len(class_names))\n",
        "model = model_cpu.to(device)"
      ],
      "metadata": {
        "id": "jSo6vVWFbNLD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rand_image_tensor = torch.randn(size=(1,28,28))"
      ],
      "metadata": {
        "id": "a3JST7JSLlbK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trick!\n",
        "model(rand_image_tensor.unsqueeze(0).to(device))"
      ],
      "metadata": {
        "id": "-42HbC1fLvwY",
        "outputId": "3fa73e80-8727-4ac2-9b28-6cd2c11cc7dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0366, -0.0940,  0.0686, -0.0485,  0.0068,  0.0290,  0.0132,  0.0084,\n",
              "         -0.0030, -0.0185]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functionizing training and evaluation/testing loops"
      ],
      "metadata": {
        "id": "E8CnWbgQMX-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model:torch.nn.Module,\n",
        "        data_loader:torch.utils.data.DataLoader,\n",
        "        loss_fn:torch.nn.Module,\n",
        "        optimizer:torch.optim.Optimizer,\n",
        "        accuracy_fn,\n",
        "        device:torch.device=device):\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  # Put model into training mode\n",
        "  model.train()\n",
        "\n",
        "  # Add a loop through the training batches\n",
        "  for batch, (X, y) in enumerate(data_loader):\n",
        "    # Put data on target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # 1. Forward pass (outputs the raw logits from the model)\n",
        "    y_pred = model(X)\n",
        "\n",
        "    # 2. Calculate loss and accuracy (per batch)\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss # accumulate train loss\n",
        "    train_acc += accuracy_fn(y_true=y,\n",
        "                  y_pred=y_pred.argmax(dim=1)) # go from logits\n",
        "\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "  # Divide total train loss and acc by length of train dataloader\n",
        "  train_loss /= len(data_loader)\n",
        "  train_acc /= len(data_loader)\n",
        "  print(f\"Train loss: {train_loss:.5f} | Train acc: {train_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "jrlE-8BPMrFJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(model:torch.nn.Module,\n",
        "        data_loader:torch.utils.data.DataLoader,\n",
        "        loss_fn:torch.nn.Module,\n",
        "        accuracy_fn,\n",
        "        device:torch.device=device):\n",
        "  test_loss, test_acc = 0, 0\n",
        "\n",
        "  # Put the model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # Turn on inference model context manager\n",
        "  with torch.inference_mode():\n",
        "    for X, y in data_loader:\n",
        "      # Send the data to the target device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # 1. Forward pass\n",
        "      test_pred = model(X)\n",
        "\n",
        "      # 2. Calcualte the loss/acc\n",
        "      test_loss += loss_fn(test_pred, y)\n",
        "      test_acc += accuracy_fn(y_true=y,\n",
        "                   y_pred=test_pred.argmax(dim=1)) # go from logits -> prediction labels\n",
        "\n",
        "    # Adjust metrics and print out\n",
        "    test_loss /= len(data_loader)\n",
        "    test_acc /= len(data_loader)\n",
        "    print(f\"Test loss: {test_loss:.5f} | Test acc: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "U9zWTmsRPxQ7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model:torch.nn.Module,\n",
        "        data_loader:torch.utils.data.DataLoader,\n",
        "        loss_fn:torch.nn.Module,\n",
        "        accuracy_fn,\n",
        "        device=device):\n",
        "  loss, acc = 0, 0\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X, y in tqdm(data_loader):\n",
        "      # Make our data device agnostic\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # Make predictions\n",
        "      y_pred = model(X)\n",
        "\n",
        "      # Accumulate the loss and acc values per batch\n",
        "      loss += loss_fn(y_pred, y)\n",
        "      acc += accuracy_fn(y_true=y,\n",
        "                 y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "    # Scale loss and acc to find the average loss/acc per batch\n",
        "    loss /= len(data_loader)\n",
        "    acc /= len(data_loader)\n",
        "\n",
        "  return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "       \"model_loss\": loss.item(),\n",
        "       \"model_acc\": acc}"
      ],
      "metadata": {
        "id": "5lZrmjnSeknP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_train_time(start: float,\n",
        "           end: float,\n",
        "           device: torch.device = None):\n",
        "  total_time = end - start\n",
        "  print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "  return total_time"
      ],
      "metadata": {
        "id": "2L-dRoKwaepm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup loss function/eval metrics/optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model.parameters(),\n",
        "               lr=0.1)\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper functions from Learn PyTorch repo\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download...\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/refs/heads/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)\n",
        "\n",
        "# Import accuracy metric\n",
        "from helper_functions import accuracy_fn\n"
      ],
      "metadata": {
        "id": "sPlwrYuNXGVe",
        "outputId": "f854e145-41db-45dd-fc32-2a1d5151c08e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "helper_functions.py already exists, skipping download...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Measure time\n",
        "from timeit import default_timer as timer\n",
        "train_time_start_model_on_cpu = timer()\n",
        "\n",
        "# Train and test model\n",
        "epochs = 5\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epoch}\\n-------\")\n",
        "  train_step(model=model,\n",
        "        data_loader=train_dataloader,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        accuracy_fn=accuracy_fn,\n",
        "        device=device_cpu)\n",
        "  test_step(model=model,\n",
        "       data_loader=test_dataloader,\n",
        "       loss_fn=loss_fn,\n",
        "       accuracy_fn=accuracy_fn,\n",
        "       device=device_cpu)\n",
        "\n",
        "  train_time_end_model_on_cpu = timer()\n",
        "  total_train_timr_model_on_cpu = print_train_time(start=train_time_start_model_on_cpu,\n",
        "                        end=train_time_end_model_on_cpu,\n",
        "                        device=device_cpu)\n",
        "\n",
        "\n",
        "train_time_start_model_on_gpu = timer()\n",
        "\n",
        "# Train and test model\n",
        "epochs = 5\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epoch}\\n-------\")\n",
        "  train_step(model=model,\n",
        "        data_loader=train_dataloader,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        accuracy_fn=accuracy_fn,\n",
        "        device=device)\n",
        "  test_step(model=model,\n",
        "       data_loader=test_dataloader,\n",
        "       loss_fn=loss_fn,\n",
        "       accuracy_fn=accuracy_fn,\n",
        "       device=device)\n",
        "\n",
        "  train_time_end_model_on_cpu = timer()\n",
        "  total_train_timr_model_on_cpu = print_train_time(start=train_time_start_model_on_cpu,\n",
        "                        end=train_time_end_model_on_cpu,\n",
        "                        device=device)\n",
        "\n",
        "  model_cpu_results = eval_model(model=model_cpu,\n",
        "                   data_loader=test_dataloader,\n",
        "                   loss_fn=loss_fn,\n",
        "                   accuracy_fn=accuracy_fn,\n",
        "                   device=device_cpu)\n",
        "  model_gpu_results = eval_model(model=model_cpu,\n",
        "                   data_loader=test_dataloader,\n",
        "                   loss_fn=loss_fn,\n",
        "                   accuracy_fn=accuracy_fn,\n",
        "                   device=device)\n",
        "  print(model_cpu_results)\n",
        "  print(model_gpu_results)"
      ],
      "metadata": {
        "id": "Q1OnG7C_ZXCN",
        "outputId": "252a21fe-2e7c-4ec1-a6e8-797b5e4586c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396,
          "referenced_widgets": [
            "e59f392ab0a4456aab9ad16272f04010",
            "877ee0af6b744f979394fc6ee0a19ab2",
            "6485855357c24b50a603e5ffa00b441c",
            "d4d3b5da139f4f56bf0dd5d0f3c7d5ab",
            "a889b8e790e24c9baba4a5e379ac9e63",
            "dff622a6496248c6bd19831da7bd3d56",
            "8151abf488cd42bc8c5a0f9351c5e659",
            "9a2901ddea88411a9c8ca93be0b658ef",
            "d2d2016cd721480b89183ab1ad7ed3a6",
            "8d002d7967284ddc99ab266787d3fc4b",
            "c20ada00198748ecad0c5229419d6916"
          ]
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e59f392ab0a4456aab9ad16272f04010"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "-------\n",
            "Train loss: 0.08353 | Train acc: 97.40%\n",
            "Train time on cpu: 50.418 seconds\n",
            "Epoch: 1\n",
            "-------\n",
            "Train loss: 0.06446 | Train acc: 98.00%\n",
            "Train time on cpu: 99.276 seconds\n",
            "Epoch: 2\n",
            "-------\n",
            "Train loss: 0.05489 | Train acc: 98.27%\n",
            "Train time on cpu: 147.996 seconds\n",
            "Epoch: 3\n",
            "-------\n",
            "Train loss: 0.04916 | Train acc: 98.42%\n",
            "Train time on cpu: 196.856 seconds\n",
            "Epoch: 4\n",
            "-------\n",
            "Train loss: 0.04521 | Train acc: 98.59%\n",
            "Train time on cpu: 251.176 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Make predictions using your trained model and visualize at least 5 of them comparing the prediciton to the target label."
      ],
      "metadata": {
        "id": "w1CsHhPpxp1w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_YGgZvSobNxu"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Plot a confusion matrix comparing your model's predictions to the truth labels."
      ],
      "metadata": {
        "id": "qQwzqlBWxrpG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vSrXiT_AbQ6e"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Create a random tensor of shape `[1, 3, 64, 64]` and pass it through a `nn.Conv2d()` layer with various hyperparameter settings (these can be any settings you choose), what do you notice if the `kernel_size` parameter goes up and down?"
      ],
      "metadata": {
        "id": "lj6bDhoWxt2y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "leCTsqtSbR5P"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. Use a model similar to the trained `model_2` from notebook 03 to make predictions on the test [`torchvision.datasets.FashionMNIST`](https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html) dataset.\n",
        "* Then plot some predictions where the model was wrong alongside what the label of the image should've been.\n",
        "* After visualing these predictions do you think it's more of a modelling error or a data error?\n",
        "* As in, could the model do better or are the labels of the data too close to each other (e.g. a \"Shirt\" label is too close to \"T-shirt/top\")?"
      ],
      "metadata": {
        "id": "VHS20cNTxwSi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78a8LjtdbSZj"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}